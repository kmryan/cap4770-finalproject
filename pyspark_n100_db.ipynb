{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "def time_func(func):\n",
    "    def inner():\n",
    "        stime = dt.now()\n",
    "        result = func()\n",
    "        ftime = dt.now()\n",
    "        print (f\"Completed in {(ftime - stime).total_seconds()} seconds\")\n",
    "        return result\n",
    "    return inner\n",
    "\n",
    "db_path = './n100.db'\n",
    "\n",
    "#path = './2020-07-15/document_parses/pdf_json/*.json'\n",
    "#path = './n10k_pdfs/*.json'\n",
    "path = './n100_pdfs/*.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test global time\n",
    "gstart = dt.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b2da325f960f:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://spark-master:7077 appName=pyspark-notebook>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"2048m\").\\\n",
    "        getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "sw_fileids = stopwords.fileids()\n",
    "sw_dict = {lang:stopwords.words(lang) for lang in sw_fileids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "\n",
    "if os.path.exists(db_path):\n",
    "    os.remove(db_path)\n",
    "\n",
    "create_papers_table = '''\n",
    "    CREATE TABLE papers (\n",
    "        paper_id text PRIMARY KEY NOT NULL,\n",
    "        title text NOT NULL\n",
    "    );\n",
    "'''\n",
    "\n",
    "create_sentences_table = '''\n",
    "    CREATE TABLE sentences (\n",
    "        paper_id tetx NOT NULL,\n",
    "        sentence_number integer NOT NULL,\n",
    "        sentence text,\n",
    "        PRIMARY KEY (paper_id, sentence_number)\n",
    "        FOREIGN KEY (paper_id)\n",
    "            REFERENCES papers (paper_id)\n",
    "    );\n",
    "'''\n",
    "\n",
    "try:\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    conn.execute(create_papers_table)\n",
    "    conn.execute(create_sentences_table)\n",
    "\n",
    "    conn.commit()\n",
    "    \n",
    "    print(\"OK!\")\n",
    "\n",
    "except sqlite3.Error as error:\n",
    "    print(error)\n",
    "\n",
    "finally:\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_paper_json(raw_json):\n",
    "    try:\n",
    "        res = json.loads(raw_json)\n",
    "    except:\n",
    "        res = ''\n",
    "    \n",
    "    return res\n",
    "\n",
    "def parse_paper_json(loaded_json):\n",
    "    \"\"\"\n",
    "    Builds and returns a dict containing the paper_id, title, and\n",
    "    body_text (joined sections) for further processing\n",
    "\n",
    "    :param paper_json: loaded JSON object\n",
    "    :return: dict containing targeted information\n",
    "    \"\"\"\n",
    "    return \\\n",
    "        {\n",
    "            'paper_id':     loaded_json['paper_id'],\n",
    "            'title':        loaded_json['metadata']['title'],\n",
    "            'body_text':    \" \".join([x['text'] for x in loaded_json['body_text']])\n",
    "        }\n",
    "\n",
    "def text_lang_likely(text):\n",
    "    \"\"\"\n",
    "    Compares tokenized text to set of stopwords for each language contained\n",
    "    within the NLTK stopwords corpus and outputs the likely language based on\n",
    "    the number of common words.\n",
    "\n",
    "    Adapted from http://blog.alejandronolla.com/2013/05/15/detecting-text-language-with-python-and-nltk/\n",
    "\n",
    "    :param text: body of text\n",
    "    :return: most likely language of text\n",
    "    \"\"\"\n",
    "    wp_words = set(wd.lower() for wd in wordpunct_tokenize(text))\n",
    "    lang_scores = {}\n",
    "    for lang in sw_fileids:\n",
    "        sw_set = set(sw_dict[lang])\n",
    "        intersection = wp_words & sw_set\n",
    "        lang_scores[lang] = len(intersection)\n",
    "    return max(lang_scores, key=lang_scores.get) # return language with highest score\n",
    "\n",
    "def lang_likely_wrapper(data):\n",
    "    import nltk\n",
    "    data.update({'lang': text_lang_likely(data['body_text'])})\n",
    "    return data\n",
    "\n",
    "def process_sentences(data):\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "        \n",
    "    sentences = sent_tokenize(data['body_text'])\n",
    "    \n",
    "    return [{\n",
    "        'paper_id': data['paper_id'],\n",
    "        'sentence_number': i,\n",
    "        'sentence': sentence\n",
    "    } for i,sentence in enumerate(sentences)]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed in 1.697108 seconds\n"
     ]
    }
   ],
   "source": [
    "@time_func\n",
    "def get_rdd():\n",
    "    return sc \\\n",
    "            .wholeTextFiles(path).values() \\\n",
    "            .map(load_paper_json) \\\n",
    "            .filter(lambda x: x != '') \\\n",
    "            .map(parse_paper_json) \\\n",
    "            .map(lang_likely_wrapper) \\\n",
    "            .filter(lambda x: x['lang'] == 'english')\n",
    "\n",
    "rdd = get_rdd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed in 4.606717 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done with papers!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_paper = \"INSERT INTO papers (paper_id, title) VALUES (?, ?);\"\n",
    "\n",
    "@time_func\n",
    "def process_papers():\n",
    "    paper_table_data = rdd.map(lambda x: {'paper_id':x['paper_id'], 'title':x['title']}).collect()\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    for paper_data in paper_table_data:\n",
    "        conn.execute(add_paper, (paper_data['paper_id'], paper_data['title']))\n",
    "    \n",
    "    conn.commit()\n",
    "\n",
    "    conn.close()\n",
    "    \n",
    "    return \"Done with papers!\"\n",
    "    \n",
    "process_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed in 0.14097 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done with sentences!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_sentence = \"INSERT INTO sentences (paper_id, sentence_number, sentence) VALUES (?, ?, ?)\"\n",
    "\n",
    "sentence_table_data = rdd.map(process_sentences).reduce(lambda x,y: x+y)\n",
    "\n",
    "@time_func\n",
    "def process_sentences():\n",
    "\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    for sentence_data in sentence_table_data:\n",
    "        conn.execute(add_sentence, (sentence_data['paper_id'], sentence_data['sentence_number'], sentence_data['sentence']))\n",
    "    \n",
    "    conn.commit()\n",
    "\n",
    "    conn.close()\n",
    "    \n",
    "    return \"Done with sentences!\"\n",
    "    \n",
    "process_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook took: 15.508026 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "# Report global run time\n",
    "gfinish = dt.now()\n",
    "\n",
    "print(f\"Notebook took: {(gfinish - gstart).total_seconds()} seconds to run.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
