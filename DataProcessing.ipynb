{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# COVID-411 Project\n",
    "## Data & Pre-Processing Report + Code\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this project, the CORD-19 from 2020-07-15 is utilized. The full\n",
    "dataset consists of ```85,451``` parsed PDF-origin papers and ```61,966``` parsed XML-origin\n",
    "papers from PubMed Central, along with supporting and metadata files.\n",
    "\n",
    "For the scope of the project it became necessary to narrow the focus of the data involved,\n",
    "so we elected to utilize only the PDF-origin parses to create the final product. This document\n",
    "will briefly outline the data and pre-processing steps taken to prepare the data for generating\n",
    "our model.\n",
    "\n",
    "A local PySpark environment in standalone mode was ultimately utilized for simplicity, as it was not\n",
    "feasible for these authors to constantly spin up and down cloud services during development. Initially, a more\n",
    "complex distributed Apache Spark cluster was trialed on a single machine using Docker, including\n",
    "a HDFS volume and a Master with several Worker nodes and a JupyterLab instance for running code.\n",
    "However, while effective (particularly for parallel processing) this approach was ultimately abandoned due to\n",
    "difficult interoperability with later stages of the project, along with performance issues (mostly related to the\n",
    "Windows Docker Desktop WSL2 backend and its underlying memory issues.) A copy of the Docker files (and attributions as\n",
    "to where they were derived from) will be included in the project codebase for reference.\n",
    "\n",
    "The use of the standalone PySpark environment presented some challenges with memory management. Processing the full\n",
    "dataset via a Resilient Distributed Dataset (RDD) and then collecting all at once tended to trigger an\n",
    "OutOfMemoryException for the Java Heap. To\n",
    "mitigate this the data was first partitioned on disk into 10,000 file chunks and processed sequentially, the results of\n",
    "which were written to an intermediate database.\n",
    "Transitioning from use of map to foreach to perform the task resulted in a more\n",
    "stable running environment, however we kept the partitioning as it helped to recover in the case of a mid-process crash.\n",
    "The first iteration using the distributed cluster environment\n",
    "utilized a PostgreSQL server on a Docker image on the same virtual network as the Spark cluster. This network\n",
    "environment allowed for worker nodes to commit the processed data to the database in parallel streams. However, it\n",
    "was determined a simpler approach would need to be used for our purposes and we switched to a SQLite database as it\n",
    "would give a single database file that could be more easily manipulated. Since the use of a file-based database\n",
    "platform with frequent locking would negate the benefit of parallelizing the streaming of data into the database, the\n",
    "Standalone Spark environment worked well for our purposes\n",
    "\n",
    "The code (and output) for partitioning and initial data pre-processing is included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition directories:  ['p0', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8']\n"
     ]
    }
   ],
   "source": [
    "# PARTITIONING STEP\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "SRC_PATH = '../data/2020-07-15/document_parses/pdf_json/'\n",
    "DEST_PATH = './data/'\n",
    "\n",
    "files = os.listdir(SRC_PATH)\n",
    "\n",
    "part_dirs = os.listdir(DEST_PATH)\n",
    "\n",
    "print(\"Partition directories: \", part_dirs)\n",
    "\n",
    "nfiles = len(files)\n",
    "\n",
    "# There will be 9 total partitions (0-8) for 10,000 file chunks\n",
    "for i in range(9):\n",
    "    cdir = part_dirs[i]\n",
    "\n",
    "    for j in range(10000):\n",
    "        fn = (i*10000) + j # file number\n",
    "\n",
    "        if fn == nfiles:\n",
    "            break\n",
    "\n",
    "        cfile = files[fn]\n",
    "\n",
    "        # Show the progress of the partitioning\n",
    "        print(f'Processing partition {i+1} of 9 [{cdir}]: Processing file {fn+1} [{cfile}] of {nfiles}.', end='\\r')\n",
    "\n",
    "        # Also considered hardlinking rather than copying, but diskspace was not a concern for this part\n",
    "        shutil.copy(SRC_PATH+cfile, DEST_PATH+cdir+'/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<SparkContext master=local[*] appName=PySparkShell>",
      "text/html": "\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://xan-desk.mshome.net:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        "
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ENVIRONMENT SETUP AND INITIAL PRE-PROCESSING PREPARATIONS\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "nbstime = dt.now()\n",
    "\n",
    "FILES = '*.json'\n",
    "PARTS = '../data'\n",
    "\n",
    "DB = './covid411.db'\n",
    "\n",
    "parts = os.listdir(PARTS)\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.\\\n",
    "#         builder.\\\n",
    "#         appName(\"pyspark-notebook\").\\\n",
    "#         master(\"spark://spark-master:7077\").\\\n",
    "#         config(\"spark.executor.memory\", \"1024m\").\\\n",
    "#         getOrCreate()\n",
    "\n",
    "# sc = spark.sparkContext\n",
    "\n",
    "# Present in the pyspark environment that spawned the Jupyter server\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are currently 0 papers and 0 sentences present in the database.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "# Remove database file if exists\n",
    "if os.path.isfile(DB):\n",
    "    os.remove(DB)\n",
    "\n",
    "def get_db_conn(db):\n",
    "\n",
    "    return sqlite3.connect(db)\n",
    "\n",
    "\n",
    "def show_record_counts(cur):\n",
    "\n",
    "    q_papers = \"SELECT COUNT(*) FROM papers;\"\n",
    "    q_sentences = \"SELECT COUNT(*) FROM sentences;\"\n",
    "\n",
    "    cur.execute(q_papers)\n",
    "    n_papers = cur.fetchone()[0]\n",
    "\n",
    "    cur.execute(q_sentences)\n",
    "    n_sentences = cur.fetchone()[0]\n",
    "\n",
    "    print (f\"There are currently {n_papers} papers and {n_sentences} sentences present in the database.\")\n",
    "\n",
    "\n",
    "# Initialize DB\n",
    "\n",
    "with get_db_conn(DB) as conn:\n",
    "\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    cur.execute('''\n",
    "        CREATE TABLE papers (\n",
    "            paper_id text PRIMARY KEY NOT NULL,\n",
    "            title text NOT NULL\n",
    "        );\n",
    "    ''')\n",
    "\n",
    "    cur.execute('''\n",
    "        CREATE TABLE sentences (\n",
    "            paper_id text NOT NULL,\n",
    "            sentence_number integer NOT NULL,\n",
    "            sentence text,\n",
    "            PRIMARY KEY (paper_id, sentence_number),\n",
    "            CONSTRAINT fk_paper\n",
    "                FOREIGN KEY (paper_id)\n",
    "                    REFERENCES papers (paper_id)\n",
    "        );\n",
    "    ''')\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    show_record_counts(cur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The bulk of the processing is shown in the code below. The first task involved was to load the JSON files and parse\n",
    "them into a Spark RDD. Next, the following pieces of data were targeted:\n",
    "\n",
    "| Field | Description\n",
    "| :- | :-\n",
    "| **```paper_id```** | a unique id hash\n",
    "| **```title```** | title of the paper\n",
    "| **```body_text```** | the main text of the paper, consisting of all sections\n",
    "\n",
    "This information was processed and represented as a list of Python dictionaries via the RDD map function. Then, NLTK\n",
    "was utilized to detect the written language of the paper by examining stopwords, the aim of which was to filter out all\n",
    "non-English papers for the purposes of this project. Finally, the sentences of the ```body_text``` were split using an\n",
    "NLTK tokenizer and all the relevant data was placed into the intermediate database. This process was broken up by the\n",
    "partitioning paradigm discussed in the first section of this report.\n",
    "\n",
    "The code (and output) for these tasks follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# DATA PROCESSING HELPER FUNCTIONS\n",
    "\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "def load_paper_json(raw_json):\n",
    "\n",
    "    # This was necessary because there was a file that would not\n",
    "    # parse properly and would error the whole operation\n",
    "\n",
    "    try:\n",
    "        res = json.loads(raw_json)\n",
    "    except:\n",
    "        res = ''\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def parse_paper_json(loaded_json):\n",
    "    \"\"\"\n",
    "    Builds and returns a dict containing the paper_id, title, and\n",
    "    body_text (joined sections) for further processing\n",
    "\n",
    "    :param paper_json: loaded JSON object\n",
    "    :return: dict containing targeted information\n",
    "    \"\"\"\n",
    "    return \\\n",
    "        {\n",
    "            'paper_id':     loaded_json['paper_id'],\n",
    "            'title':        loaded_json['metadata']['title'],\n",
    "            'body_text':    \" \".join([x['text'] for x in loaded_json['body_text']])\n",
    "        }\n",
    "\n",
    "\n",
    "sw_fileids = stopwords.fileids()\n",
    "sw_dict = {lang:stopwords.words(lang) for lang in sw_fileids}\n",
    "\n",
    "def text_lang_likely(text):\n",
    "    \"\"\"\n",
    "    Compares tokenized text to set of stopwords for each language contained\n",
    "    within the NLTK stopwords corpus and outputs the likely language based on\n",
    "    the number of common words.\n",
    "\n",
    "    Adapted from http://blog.alejandronolla.com/2013/05/15/detecting-text-language-with-python-and-nltk/\n",
    "\n",
    "    :param text: body of text\n",
    "    :return: most likely language of text\n",
    "    \"\"\"\n",
    "    wp_words = set(wd.lower() for wd in wordpunct_tokenize(text))\n",
    "    lang_scores = {}\n",
    "    for lang in sw_fileids:\n",
    "        sw_set = set(sw_dict[lang])\n",
    "        intersection = wp_words & sw_set\n",
    "        lang_scores[lang] = len(intersection)\n",
    "    return max(lang_scores, key=lang_scores.get) # return language with highest score\n",
    "\n",
    "\n",
    "def lang_likely_wrapper(data):\n",
    "    #import nltk ; was initially necessary to load NLTK on each worker node\n",
    "\n",
    "    # Adds a dictionary field for the detected language of the paper\n",
    "    data.update({'lang': text_lang_likely(data['body_text'])})\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def process_sentences(body_text):\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "\n",
    "    # Gets a list of sentences using the NLTK sentence tokenizer on the whole text body\n",
    "    sentences = sent_tokenize(body_text)\n",
    "\n",
    "    # Returns a list of dictionaries assigning a number to each sentence for use as\n",
    "    # a composite key in the intermediate database along with paper_id\n",
    "    return [{\n",
    "        'sentence_number': i,\n",
    "        'sentence': sentence\n",
    "    } for i,sentence in enumerate(sentences)]\n",
    "\n",
    "# Define the SQL statements for inserting data into the intermediate database\n",
    "add_paper = \"INSERT INTO papers (paper_id, title) VALUES (?, ?);\"\n",
    "add_sentence = \"INSERT INTO sentences (paper_id, sentence_number, sentence) VALUES (?, ?, ?);\"\n",
    "\n",
    "def db_transact(row):\n",
    "    \"\"\"\n",
    "    Intended to be used inside a map function within a Spark RDD foreach call\n",
    "    \"\"\"\n",
    "    with get_db_conn(DB) as conn:\n",
    "\n",
    "        conn.cursor().execute(add_paper, (row['paper_id'], row['title']))\n",
    "\n",
    "        for sentence_data in process_sentences(row['body_text']):\n",
    "            conn.cursor().execute(add_sentence, (row['paper_id'], sentence_data['sentence_number'], sentence_data['sentence']))\n",
    "\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def db_transact_wrapper(x):\n",
    "\n",
    "    # Will output to the PySpark console so I can keep track\n",
    "    print(f\"Processing paper: {x['paper_id']}\", end='\\r')\n",
    "\n",
    "    db_transact(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition [p0] completed in 154.623591 seconds\n",
      "Partition [p1] completed in 193.394659 seconds\n",
      "Partition [p2] completed in 212.696997 seconds\n",
      "Partition [p3] completed in 202.906467 seconds\n",
      "Partition [p4] completed in 202.743589 seconds\n",
      "Partition [p5] completed in 202.516695 seconds\n",
      "Partition [p6] completed in 200.014015 seconds\n",
      "Partition [p7] completed in 202.893136 seconds\n",
      "Partition [p8] completed in 93.316841 seconds\n",
      "-----------\n",
      "Data processing completed in 1674.647034 seconds\n"
     ]
    }
   ],
   "source": [
    "# MAIN PROCESS DRIVER\n",
    "\n",
    "for part in parts:\n",
    "        stime_in = dt.now()\n",
    "\n",
    "        sc \\\n",
    "            .wholeTextFiles(f'{PARTS}/{part}/{FILES}').values() \\\n",
    "            .map(load_paper_json) \\\n",
    "            .filter(lambda x: x != '') \\\n",
    "            .map(parse_paper_json) \\\n",
    "            .map(lang_likely_wrapper) \\\n",
    "            .filter(lambda x: x['lang'] == 'english') \\\n",
    "            .foreach(db_transact_wrapper)\n",
    "\n",
    "        print (f\"Partition [{part}] completed in {(dt.now() - stime_in).total_seconds()} seconds\")\n",
    "\n",
    "print (\"-----------\")\n",
    "print (f\"Data processing completed in {(dt.now() - nbstime).total_seconds()} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are currently 82626 papers and 15081367 sentences present in the database.\n"
     ]
    }
   ],
   "source": [
    "with get_db_conn(DB) as conn:\n",
    "    show_record_counts(conn.cursor())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python37764bit05073a4964d44a96ada07f82209626d1",
   "language": "python",
   "display_name": "Python 3.7.7 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}